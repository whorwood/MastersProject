{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"project-binary-ex.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNiXscSWnJKpt3xmq4m2BFN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vnAyhodJDXe-"},"outputs":[],"source":["# Import all the neccessary libraries\n","from tensorflow import keras\n","import tensorflow as tf\n","import librosa\n","from librosa import display\n","import os\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn import utils\n","import re\n","from keras.models import model_from_json"]},{"cell_type":"code","source":["# Read in the text files containing the onset times and only keep the first column\n","f = open('./aug_bridge_tap.txt', \"r\")\n","lines = f.readlines()\n","beats1 = []\n","\n","for line in lines:\n","      line = line.strip()\n","      x = re.split(r'\\t+', line)\n","      beats1.append(float(x[0]))\n","f.close()\n","\n","f = open('./aug_bridge_tap_2.txt', \"r\")\n","lines = f.readlines()\n","beats2 = []\n","\n","for line in lines:\n","      line = line.strip()\n","      x = re.split(r'\\t+', line)\n","      beats2.append(float(x[0]))\n","f.close()\n","\n","f = open('./aug_bridge_tap_3.txt', \"r\")\n","lines = f.readlines()\n","beats3 = []\n","\n","for line in lines:\n","      line = line.strip()\n","      x = re.split(r'\\t+', line)\n","      beats3.append(float(x[0]))\n","f.close()\n","\n","f = open('./aug_bridge_tap_4.txt', \"r\")\n","lines = f.readlines()\n","beats4 = []\n","\n","for line in lines:\n","      line = line.strip()\n","      x = re.split(r'\\t+', line)\n","      beats4.append(float(x[0]))\n","f.close()"],"metadata":{"id":"CsNoVvVWDc1T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initiate the arrays\n","Y = []\n","X = []\n","\n","# Read in the audio files and convert them to the frequency domain through STFT\n","x1, sr = librosa.load('./aug_bridge_tap.wav', sr=None)\n","X_1 = abs(librosa.stft(x1, n_fft=1024))\n","\n","x2, sr = librosa.load('./aug_bridge_tap_2.wav', sr=None)\n","X_2 = abs(librosa.stft(x2, n_fft=1024))\n","\n","x3, sr = librosa.load('./aug_bridge_tap_3.wav', sr=None)\n","X_3 = abs(librosa.stft(x3, n_fft=1024))\n","\n","x4, sr = librosa.load('./aug_bridge_tap_4.wav', sr=None)\n","X_4 = abs(librosa.stft(x4, n_fft=1024))\n","\n","x5, sr = librosa.load('./aug_lower-bout_tap.wav', sr=None)\n","X_5 = abs(librosa.stft(x5, n_fft=1024))\n","\n","x6, sr = librosa.load('./aug_side_slap.wav', sr=None)\n","X_6 = abs(librosa.stft(x6, n_fft=1024))\n","\n","x7, sr = librosa.load('./aug_lower-bout_tap_2.wav', sr=None)\n","X_7 = abs(librosa.stft(x7, n_fft=1024))\n","\n","x8, sr = librosa.load('./aug_side_slap_2.wav', sr=None)\n","X_8 = abs(librosa.stft(x8, n_fft=1024))\n","\n","# Convert the onset times into the nearest integer window number\n","onsets1 = []\n","for i in range(len(beats1)):\n","  onsets1.append(int(np.floor((beats1[i]*44100)/256)))\n","\n","onsets2 = []\n","for i in range(len(beats2)):\n","  onsets2.append(int(np.floor((beats2[i]*44100)/256)))\n","\n","onsets3 = []\n","for i in range(len(beats3)):\n","  onsets3.append(int(np.floor((beats3[i]*44100)/256)))\n","\n","onsets4 = []\n","for i in range(len(beats4)):\n","  onsets4.append(int(np.floor((beats4[i]*44100)/256)))\n","\n","# Append each 15 window long section in turn and check whether there is an onset here\n","for i in range(len(X_1[0])-14):\n","  X_temp = []\n","  for x in range(len(X_1)):\n","    X_temp.append(X_1[x][i:i+15])\n","  X.append(X_temp)\n","  if (i+7 in onsets1) or (i+6 in onsets1) or (i+8 in onsets1) or (i+5 in onsets1) or (i+9 in onsets1) or (i+4 in onsets1) or (i+10 in onsets1) or (i+3 in onsets1) or (i+11 in onsets1) or (i+2 in onsets1) or (i+12 in onsets1) or (i+1 in onsets1) or (i+13 in onsets1) or (i in onsets1) or (i+14 in onsets1):\n","    Y.append(1)\n","  else:\n","    Y.append(0)\n","\n","for i in range(len(X_2[0])-14):\n","  X_temp = []\n","  for x in range(len(X_2)):\n","    X_temp.append(X_2[x][i:i+15])\n","  X.append(X_temp)\n","  if (i+7 in onsets2) or (i+6 in onsets2) or (i+8 in onsets2) or (i+5 in onsets2) or (i+9 in onsets2) or (i+4 in onsets2) or (i+10 in onsets2) or (i+3 in onsets2) or (i+11 in onsets2) or (i+2 in onsets2) or (i+12 in onsets2) or (i+1 in onsets2) or (i+13 in onsets2) or (i in onsets2) or (i+14 in onsets2):\n","    Y.append(1)\n","  else:\n","    Y.append(0)\n","\n","for i in range(len(X_3[0])-14):\n","  X_temp = []\n","  for x in range(len(X_3)):\n","    X_temp.append(X_3[x][i:i+15])\n","  X.append(X_temp)\n","  if (i+7 in onsets3) or (i+6 in onsets3) or (i+8 in onsets3) or (i+5 in onsets3) or (i+9 in onsets3) or (i+4 in onsets3) or (i+10 in onsets3) or (i+3 in onsets3) or (i+11 in onsets3) or (i+2 in onsets3) or (i+12 in onsets3) or (i+1 in onsets3) or (i+13 in onsets3) or (i in onsets3) or (i+14 in onsets3):\n","    Y.append(1)\n","  else:\n","    Y.append(0)\n","\n","for i in range(len(X_4[0])-14):\n","  X_temp = []\n","  for x in range(len(X_4)):\n","    X_temp.append(X_4[x][i:i+15])\n","  X.append(X_temp)\n","  if (i+7 in onsets4) or (i+6 in onsets4) or (i+8 in onsets4) or (i+5 in onsets4) or (i+9 in onsets4) or (i+4 in onsets4) or (i+10 in onsets4) or (i+3 in onsets4) or (i+11 in onsets4) or (i+2 in onsets4) or (i+12 in onsets4) or (i+1 in onsets4) or (i+13 in onsets4) or (i in onsets4) or (i+14 in onsets4):\n","    Y.append(1)\n","  else:\n","    Y.append(0)\n","\n","for i in range(len(X_5[0])-14):\n","  X_temp = []\n","  for x in range(len(X_5)):\n","    X_temp.append(X_5[x][i:i+15])\n","  X.append(X_temp)\n","  Y.append(0)\n","\n","for i in range(len(X_6[0])-14):\n","  X_temp = []\n","  for x in range(len(X_6)):\n","    X_temp.append(X_6[x][i:i+15])\n","  X.append(X_temp)\n","  Y.append(0)\n","\n","for i in range(len(X_7[0])-14):\n","  X_temp = []\n","  for x in range(len(X_7)):\n","    X_temp.append(X_7[x][i:i+15])\n","  X.append(X_temp)\n","  Y.append(0)\n","\n","for i in range(len(X_8[0])-14):\n","  X_temp = []\n","  for x in range(len(X_8)):\n","    X_temp.append(X_8[x][i:i+15])\n","  X.append(X_temp)\n","  Y.append(0)"],"metadata":{"id":"8T3zGoojDvkK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Shuffle the dataset \n","X, Y = utils.shuffle(X, Y)"],"metadata":{"id":"eTdenTg-D_g8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reshape the dataset to a format compatible with TensorFlow\n","X_tensor = np.reshape(X,(len(X),513,15,1))\n","Y_tensor = np.reshape(Y,(len(Y),1))"],"metadata":{"id":"QnanwQG6EABS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the neural network model layers\n","classifier = tf.keras.Sequential([                       \n","  tf.keras.layers.Conv2D(10, (3,7), activation='tanh', use_bias=True,),\n","  tf.keras.layers.MaxPooling2D((3,1)),\n","  tf.keras.layers.Conv2D(20, (3,3), activation='tanh', use_bias=True,),\n","  tf.keras.layers.MaxPooling2D((3,1)),\n","  tf.keras.layers.Flatten(),\n","  tf.keras.layers.Dense(256, activation='sigmoid'),\n","  tf.keras.layers.Dense(1, activation='sigmoid')\n","])"],"metadata":{"id":"HZxxbjcNEBug"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compile the model\n","classifier.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n","              optimizer='adam')"],"metadata":{"id":"NJ8Of4prEFHS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the model \n","history = classifier.fit(X_tensor[0:round(0.8*len(X_tensor))], Y_tensor[0:round(0.8*len(Y_tensor))], epochs=30)\n","plt.plot(history.history['loss'])\n","plt.title('model loss') \n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()"],"metadata":{"id":"2WuI8SosEGlw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate predictions on the remaining test set\n","predictions = classifier.predict(X_tensor[round(0.8*len(X_tensor)):len(X_tensor)-1])"],"metadata":{"id":"fW5j0OZQEJUf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set all value above the threshold to 1\n","for i in range(len(predictions)):\n","  if predictions[i]>0.5:\n","    predictions[i] = 1\n","  else:\n","    predictions[i] = 0"],"metadata":{"id":"h9H1i7hAELDr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Y_test = Y_tensor[round(0.8*len(Y_tensor)):len(Y_tensor)-1]"],"metadata":{"id":"9NrerWHKEPU8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate the number of true positive, false positives and false negatives to be used in the F-measure\n","tp = 0\n","fn = 0\n","fp = 0\n","\n","for i in range(len(predictions)):\n","  if predictions[i] == 1:\n","    if Y_test[i] == 1:\n","      tp += 1\n","    else:\n","      fp += 1\n","  elif predictions[i] == 0:\n","    if Y_test[i] == 1:\n","      fn += 1\n","\n","print(\"True Positives: \"+str(tp))\n","print(\"False Positives: \"+str(fp))\n","print(\"False Negatives: \"+str(fn))"],"metadata":{"id":"tytZ_nWjEQkk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save the model weigths in H5 format\n","classifier.save_weights(\"weights.h5\")"],"metadata":{"id":"zwveDEEGESNp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_json = classifier.to_json()\n","with open(\"model.json\", \"w\") as json_file:\n","    json_file.write(model_json)"],"metadata":{"id":"Xl82RDx6YQkk"},"execution_count":null,"outputs":[]}]}